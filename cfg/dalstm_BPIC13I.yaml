data:
  normalization: False
model:
  lstm:
    n_layers: 2
    hidden_size: 150 
    dropout: True
    dropout_prob: 0.1
uncertainty:
  dropout_approximation:
    weight_regularizer: 0.1
    dropout_regularizer: 0.1
    num_stochastic_forward_path: 50
  laplace:
    empirical_bayes: True # whether to estimate the prior precision and observation noise using empirical Bayes after training or not
    subset_of_weights: last_layer # all or last_layer or subnetwork (default: last_layer seems to be the only option)
    last_layer_name: linear2 # applicable when Laplace is applied only on the last layer
    #subnetwork_type: ModuleName # LargestMagnitude or ModuleName (only applicable for subnetwork option)
    hessian_structure: kron # full or lowrank or kron or diag (if last layer: lowrank does not work,  if subnetwork only full works)    
    link_approx: mc # : type of approximation of link function seems that the default which is exact is the best option for last layer
    n_samples: 100 # number of MC samples for approximating the predictive distribution
    pred_type: glm # glm or nn  : type of approximation of predictive distribution
    optimize_prior_precision: marglik # marglik or gridsearch
    prior_precision: 1.0 #prior precision to use for computing the covariance matrix
train:
  loss_function: mae   
  max_epochs: 100
  batch_size: 64
  early_stop.patience: 50
  early_stop.min_delta: 0
optimizer:
  type: NAdam
  base_lr: 0.001 
  eps: 1e-7  
  weight_decay: 0.0 
evaluation:
  batch_size: 64
    