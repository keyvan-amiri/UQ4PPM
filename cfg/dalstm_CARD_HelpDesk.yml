data:
    dataset: "ppm" # In our setting is always ppm
    dir: "HelpDesk"
    n_splits: 1
    num_workers: 0
    data_root: "data/datasets/Remaining_time_datasets"
    preprocessing_config: "preprocessing_config.yaml" # shared for all event logs

model:
    data_dim: 453
    x_dim: 453 # number of features after pre-processing the data
    max_len: 14 # maximum sequence length for DALSTM as guidance model
    y_dim: 1 # In our regression task at hand is always 1
    target_norm: True # whether to use normalization for target attribute or not
    cat_x: True # whether to concatanate data features (x) to the input of noise estimation network or not
    feature_dim: 128 # dimension for linear layer used in noise estimation network
    var_type: fixedlarge # fixedlarge or fixedsmall
    arch: LSTM # architecture that is used for noise estimation network
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # linear or cosine or cosine_anneal or cosine_reverse or sigmoid or jsd or quad or const
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "DALSTM"
    noise_prior_approach: median # mean: N(0,I) or median: N(normalized_median,I) only applicable if target_norm==True
    nonlinear_guidance:
        pre_train: True # if True: first train point estimator, then use it as prior for diffusion model
        joint_train: False
        n_pretrain_epochs: 300
        logging_interval: 10
        n_layers: 2 # number of LSTM layers in both guidance and noise estimation models
        hidden_size: 150 # number of neurons in LSTM layers in both guidance and noise estimation models
        dropout: True # whether to use dropout in both guidance and noise estimation models
        dropout_rate: 0.2 # dropout probability to be used in both guidance and noise estimation models
        linear_hidden_size: 5 # size of the second linear layer in DALSTM as guidance model
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.8  # for splitting original train into train and validation set for hyperparameter tuning
        train_test_ratio: 0.8 # only for holdout approach otherwise sizes are inferred from number of splits
        patience: 50 # patience for early stopping for guidance model
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 64
    n_epochs: 5000 
    n_iters: 100000 
    snapshot_freq: 1000000000 
    logging_freq: 2000 
    validation_freq: 20000 
    image_folder: 'training_image_samples'

testing:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    plot_freq: 5
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True # True : analysis for all timesteps, otherwise for the timestep as per 3 following parameters
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam" # it eas Adam
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0